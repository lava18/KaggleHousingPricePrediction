{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 81)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from preprocess import impute\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import skew\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "print(train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns where NaN values have meaning e.g. no pool etc.\n",
    "\n",
    "all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n",
    "                      test.loc[:,'MSSubClass':'SaleCondition']))\n",
    "\n",
    "\n",
    "# 3 rows where PoolQC is NA, but PoolArea >0\n",
    "temp = all_data[['PoolArea']][all_data[\"PoolQC\"].isna()]\n",
    "temp[temp.PoolArea > 0] \n",
    "\n",
    "# Everything is consistent!\n",
    "temp = all_data[['Fireplaces']][all_data[\"FireplaceQu\"].isna()]\n",
    "temp[temp.Fireplaces > 0] \n",
    "\n",
    "# 1 where GarageCond is NA, but GarageArea >0\n",
    "temp = all_data[['GarageArea']][all_data[\"GarageYrBlt\"].isna()]\n",
    "temp[temp.GarageArea > 0] \n",
    "\n",
    "all_data = impute(all_data)\n",
    "\n",
    "\n",
    "#all_data.isnull().sum()\n",
    "#list(set(train.columns.values) - set(all_data.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total = all_data.isnull().sum().sort_values(ascending=False)\n",
    "# percent = (all_data.isnull().sum()/all_data.isnull().count()).sort_values(ascending=False)\n",
    "# missing_data = pd.concat([total, percent], axis=1, keys=['Missing', 'Percent'])\n",
    "# missing_data[missing_data.Missing > 0]\n",
    "\n",
    "\n",
    "# f , ax = plt.subplots(figsize = (14,12))\n",
    "# plt.title('Correlation with Sale Price',y=1,size=16)\n",
    "# sns.heatmap(correlation,square = True,  vmax=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(set(all_data.columns.values))\n",
    "\n",
    "\n",
    "ordinal = ['LotShape', 'Utilities', 'LandSlope', 'ExterQual','ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure','BsmtFinType1', \n",
    "'BsmtFinType2', 'HeatingQC', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageFinish','GarageQual', \n",
    "'GarageCond', 'PavedDrive', 'Fence']\n",
    "\n",
    "lot = {'Reg': 4 , 'IR1': 3, 'IR2': 2, 'IR3': 1}\n",
    "util = {'AllPub': 4 , 'NoSewr': 3, 'NoSeWa': 2, 'ELO': 1}\n",
    "slope = {'Gtl': 3, 'Mod': 2, 'Sev': 1}\n",
    "expo = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'none': 0}\n",
    "bsmt_exposure = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'none': 0}\n",
    "bsmt_fin = {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'none': 0}\n",
    "electrical = {'SBrkr': 5, 'FuseA': 4, 'FuseF': 3, 'FuseP': 2, 'Mix': 1}\n",
    "functional = {'Typ': 8, 'Min1': 7, 'Min2': 6, 'Mod': 5, 'Maj1': 4, 'Maj2': 3, 'Sev': 2, 'Sal': 1}\n",
    "finish = {'Fin': 3, 'RFn': 2, 'Unf': 1, 'none': 0}\n",
    "paved = {'Y': 2, 'P': 1, 'N': 0}\n",
    "fence = {'GdPrv': 4, 'MnPrv': 3, 'GdWo': 2, 'MnWw': 1, 'none': 0}\n",
    "\n",
    "\n",
    "all_data = all_data.replace({'LotShape': lot,\n",
    "                         'LandSlope': slope,\n",
    "                         'ExterQual': expo,\n",
    "                         'ExterCond': expo,\n",
    "                         'BsmtQual': expo,\n",
    "                         'BsmtCond': expo,\n",
    "                         'BsmtExposure': bsmt_exposure,\n",
    "                         'BsmtFinType1': bsmt_fin,\n",
    "                         'BsmtFinType2': bsmt_fin,\n",
    "                         'HeatingQC': expo,\n",
    "                         'Electrical': electrical,\n",
    "                         'KitchenQual': expo,\n",
    "                         'Functional': functional,\n",
    "                         'FireplaceQu': expo,\n",
    "                         'GarageFinish': finish,\n",
    "                         'GarageQual': expo,\n",
    "                         'GarageCond': expo,\n",
    "                         'PavedDrive': paved,\n",
    "                         'Fence': fence}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = train.SalePrice\n",
    "# train = impute(train)\n",
    "# sns.jointplot(train.TotalBsmtSF, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log transform the target:\n",
    "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "\n",
    "#log transform skewed numeric features:\n",
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "\n",
    "skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n",
    "skewed_feats = skewed_feats[skewed_feats > 0.75]\n",
    "skewed_feats = skewed_feats.index\n",
    "\n",
    "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
    "\n",
    "all_data.drop('GarageYrBlt', axis = 1)\n",
    "all_data = pd.get_dummies(all_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data.loc[:, ['TotalBsmtSF', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF']]\n",
    "\n",
    "\n",
    "# HOW TO IMPUTE LOTFRONTAGE? \n",
    "#all_data.loc[:, ['LotArea', 'LotFrontage', 'LotConfig']]\n",
    "#sns.boxplot(all_data['LotConfig'].dropna(), all_data['LotFrontage'])\n",
    "\n",
    "# HOW TO IMPUTE GARAGE CONDITION?\n",
    "#703 \n",
    "# print(len(all_data[['YearBuilt','GarageYrBlt']][all_data['GarageYrBlt'] != all_data['YearBuilt']]))\n",
    "# #2216\n",
    "# print(len(all_data[['YearBuilt','GarageYrBlt']][all_data['GarageYrBlt'] == all_data['YearBuilt']]))\n",
    "\n",
    "# print(all_data[['YearBuilt','GarageYrBlt']][all_data['GarageYrBlt'] != all_data['YearBuilt']])\n",
    "# print(np.mode(all_data['GarageYrBlt'].dropna() - all_data['YearBuilt'].dropna()))\n",
    "\n",
    "#all_data[['YearBuilt','GarageYrBlt']][all_data['GarageYrBlt'].isna()]\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(10,8))\n",
    "# all_data.groupby(['GarageYrBlt','GarageCond']).agg({'GarageCond' : 'count'}).unstack().plot(ax=ax)\n",
    "\n",
    "#all_data[['GrLivArea', 'TotRmsAbvGrd']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 308)\n",
      "(1459, 308)\n",
      "(1460,)\n"
     ]
    }
   ],
   "source": [
    "X_train = all_data[:train.shape[0]]\n",
    "X_test = all_data[train.shape[0]:]\n",
    "y = train.SalePrice\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean variance of all numeric features =  2.2765364858615693\n"
     ]
    }
   ],
   "source": [
    "import sklearn.feature_selection as fs\n",
    "#Check the variance of all features\n",
    "d = dict(zip(X_train.columns, np.var(X_train[skewed_feats])))\n",
    "\n",
    "print('Mean variance of all numeric features = ' , np.mean(list(d.values())))\n",
    "#Remove features having low variance, ie- less than a threshold \n",
    "select1 = fs.VarianceThreshold(threshold=0.073).fit(X_train)\n",
    "reduced_model_fs = X_train[X_train.columns[select1.get_support(indices=True)]]\n",
    "\n",
    "\n",
    "lr = LinearRegression()\n",
    "# lr.fit(X_train[reduced_model_fs.columns], y)\n",
    "# lr_train_pred = lr.predict(X_train[reduced_model_fs.columns])\n",
    "# lr_pred = np.expm1(lr.predict(X_test[reduced_model_fs.columns]))\n",
    "# print('Linear Regression using variance FS error :', rmsle(y, lr_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENet error :  0.10344948199876194\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import ElasticNet, Lasso, LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# ENet = ElasticNet(max_iter=1000000)\n",
    "# grid_param = [{'alpha': np.logspace(-2, 3, 1000), 'l1_ratio': np.arange(0,1,0.1)}]\n",
    "# para_search = GridSearchCV(ENet, param_grid=grid_param, cv=5, return_train_score=True)\n",
    "# para_search = para_search.fit(X_train, y)\n",
    "\n",
    "# print(para_search.best_score_)\n",
    "# print(para_search.best_params_)\n",
    "\n",
    "\n",
    "e = ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3)\n",
    "ENet = make_pipeline(RobustScaler(), e)\n",
    "\n",
    "ENet.fit(X_train, y)\n",
    "enet_train_pred = ENet.predict(X_train)\n",
    "enet_pred = np.expm1(ENet.predict(X_test))\n",
    "print('ENet error : ', rmsle(y, enet_train_pred))\n",
    "\n",
    "#print(list(zip(e.coef_, X_train.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: RuntimeWarning: overflow encountered in expm1\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost error :  0.08263637106428398\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1)\n",
    "\n",
    "\n",
    "model_xgb.fit(X_train, y)\n",
    "xgb_train_pred = model_xgb.predict(X_train)\n",
    "xgb_pred = np.expm1(np.expm1(model_xgb.predict(X_test)))\n",
    "print('XGBoost error : ', rmsle(y, xgb_train_pred))\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "print('cross_val_score :', cross_val_score(model_xgb, X_train, y, cv=5))\n",
    "\n",
    "\n",
    "# grid_param = [{'gamma': np.arange(0, 1, 0.05), 'max_depth' : range(0,4,1), \n",
    "#                'reg_alpha' : np.arange(0, 1, 0.02), 'reg_lambda': np.arange(0, 1, 0.05)}]\n",
    "# para_search = GridSearchCV(model_xgb, param_grid=grid_param, cv=5, return_train_score=True)\n",
    "# para_search = para_search.fit(X_train, y)\n",
    "\n",
    "# print(para_search.best_score_)\n",
    "# print(para_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest error :  0.013530677984518126\n",
      "cross_val_score : [0.77580091 0.81327734 0.77735489 0.77900876 0.76475963]\n"
     ]
    }
   ],
   "source": [
    "# grid_param = [{'gamma': np.arange(0, 1, 0.05), 'max_depth' : range(0,4,1), \n",
    "#                'reg_alpha' : np.arange(0, 1, 0.02), 'reg_lambda': np.arange(0, 1, 0.05)}]\n",
    "# para_search = GridSearchCV(model_xgb, param_grid=grid_param, cv=5, return_train_score=True)\n",
    "\n",
    "model_gbr = rfr(random_state=0, \n",
    "        n_estimators=1000, max_depth=3,  max_features='auto')\n",
    "\n",
    "model_gbr.fit(X_train, y)\n",
    "gbr_train_pred = model_gbr.predict(X_train)\n",
    "gbr_pred = np.expm1(model_gbr.predict(X_test))\n",
    "print('Random forest error : ', rmsle(y, gbr_train_pred))\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "print('cross_val_score :', cross_val_score(model_gbr, X_train, y, cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric: [rmsle]\n",
      "\n",
      "model 0: [Pipeline]\n",
      "    ----\n",
      "    MEAN:   [0.01106772]\n",
      "\n",
      "model 1: [RandomForestRegressor]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lavanya/Desktop/NYC Bootcamp 2018/Course Material/Kaggle Housing Project/stacking.py:112: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  X_train = X_train.as_matrix()\n",
      "/Users/lavanya/Desktop/NYC Bootcamp 2018/Course Material/Kaggle Housing Project/stacking.py:113: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  X_test = X_test.as_matrix()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ----\n",
      "    MEAN:   [0.01275823]\n",
      "\n",
      "model 2: [RandomForestRegressor]\n",
      "    ----\n",
      "    MEAN:   [0.01136467]\n",
      "\n",
      "model 3: [GradientBoostingRegressor]\n",
      "    ----\n",
      "    MEAN:   [0.00997297]\n",
      "\n",
      "model 4: [GradientBoostingRegressor]\n",
      "    ----\n",
      "    MEAN:   [0.00992767]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stacking\n",
    "from stacking import stacking_regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor as gbr, RandomForestRegressor as rfr\n",
    "\n",
    "#models = [model_xgb, lr, rf1, rf2, gbm1, gbm2]\n",
    "\n",
    "models = [\n",
    "    # linear model, ElasticNet = lasso + ridge\n",
    "    ENet,\n",
    "    \n",
    "    # conservative random forst model\n",
    "    rfr(random_state=0,\n",
    "        n_estimators=1000, max_depth=6,  max_features='sqrt'),\n",
    "    \n",
    "    # aggressive random forest model\n",
    "    rfr(random_state=0, \n",
    "        n_estimators=1000, max_depth=9,  max_features='auto'),\n",
    "    \n",
    "    # conservative gbm model\n",
    "    gbr(random_state=0, learning_rate = 0.005, max_features='sqrt',\n",
    "        min_samples_leaf=15, min_samples_split=10, \n",
    "        n_estimators=3000, max_depth=3),\n",
    "    \n",
    "    # aggressive gbm model\n",
    "    gbr(random_state = 0, learning_rate = 0.01, max_features='sqrt',\n",
    "        min_samples_leaf=10, min_samples_split=5, \n",
    "        n_estimators = 1000, max_depth = 9)\n",
    "    ]\n",
    "\n",
    "#meta_model = lr(normalize=True)\n",
    "meta_model = LinearRegression(normalize=True)\n",
    "stacking_prediction = stacking_regression(models, meta_model, X_train, y, X_test,\n",
    "                                           metric=rmsle, verbose=1)\n",
    "\n",
    "stacking_prediction = np.expm1(stacking_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 2)\n",
      "(1460,)\n",
      "Ensembling error :  0.08188460340162537\n",
      "cross_val_score : [0.95677779 0.96273976 0.95442713 0.9615877  0.95225991]\n",
      "[-0.22226586  1.23130504]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1460,292) and (2,) not aligned: 292 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-480-e81b837e8206>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#Training errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mensembled_train_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_ensembled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'XGBoost training error : '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmsle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensembled_train_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \"\"\"\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0m_preprocess_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         return safe_sparse_dot(X, self.coef_.T,\n\u001b[0;32m--> 241\u001b[0;31m                                dense_output=True) + self.intercept_\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1460,292) and (2,) not aligned: 292 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "ensembling_train = pd.DataFrame({'enet': enet_train_pred,\n",
    "     'xgb': xgb_train_pred\n",
    "    })\n",
    "\n",
    "\n",
    "print(ensembling_train.shape)\n",
    "print(y.shape)\n",
    "model_ensembled = LinearRegression().fit(ensembling_train, y)\n",
    "ensembled_train_pred = model_ensembled.predict(ensembling_train)\n",
    "#ensembled_pred = np.expm1(model_ensembled.predict(X_test))\n",
    "print('Ensembling error : ', rmsle(y, ensembled_train_pred))\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "print('cross_val_score :', cross_val_score(model_ensembled, ensembling_train, y, cv=5))\n",
    "\n",
    "print(model_ensembled.coef_)\n",
    "ensembled_pred = (0.33 * enet_pred) + (0.66 * xgb_pred)\n",
    "ensembled_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "csv_file = open('submission1.csv', 'w')\n",
    "writer = csv.writer(csv_file)\n",
    "\n",
    "Id = np.arange(1461, 2920)\n",
    "writer.writerow(['Id', 'SalePrice'])\n",
    "\n",
    "\n",
    "dictionary = dict(zip(Id, gbr_pred))\n",
    "\n",
    "for key, value in dictionary.items():\n",
    "    writer.writerow([key, value])\n",
    "    csv_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
